{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqZFUpb8QbUB",
        "colab_type": "text"
      },
      "source": [
        "> [Application & tips](https://youtu.be/M6H3SGk2ddU?list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C)\n",
        "\n",
        "# Learning Rate\n",
        "\n",
        "lr: gradient를 적용시킬 때 곱하는 값\n",
        "lr이 클 수록 더 멀리, 빨리 가지만 **Overshooting**이라는 문제가 생길 수 있다.\n",
        "\n",
        "또한 너무 작다면 모델을 학습시키는데 시간이 너무 오래 걸린다.\n",
        "\n",
        "-> 최적의 lr을 찾는 것이 중요\n",
        "\n",
        "Adam Optimizer의 경우 `3e-4`가 이상적인 lr값이라고 함\n",
        "\n",
        "lr이 처음에는 큰 것이 좋지만, 나중에는 작아야 좋다.\n",
        "\n",
        "따라서 유동적으로(학습하는 과정에서) lr의 값을 바꾸기도 한다.\n",
        "\n",
        "* Step decay: N epoch or validation loss\n",
        "* Exponential decay\n",
        "  - $\\alpha = \\alpha_0 e^{-kt}$\n",
        "* 1/t decay\n",
        "   - $\\alpha = \\alpha_0 / (1 + kt)$\n",
        "\n",
        "# Data preprocessing(전처리)\n",
        "\n",
        "## Feature scaling\n",
        "\n",
        "### Standardization\n",
        "\n",
        "$\\mu: 평균 \\\\ \\sigma: 표준편차$\n",
        "$$\n",
        "x_{new} = \\frac {x - \\mu} {\\sigma}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5BqL9MCTi46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Standardization = (data - np.mean(data)) \\\n",
        " / sqrt(np.sum((data - np.mean(data)) ** 2) / np.count(data))\n",
        "\n",
        "Standardization = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNO7Oc51SjLd",
        "colab_type": "text"
      },
      "source": [
        "### Normalization\n",
        "\n",
        "$$\n",
        "x_{new} = \\frac {x - x_{min}} {x_{max} - x_{min}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q18bLSwKPPhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Normalization = (data - np.min(data, 0)) / (np.max(data, 0) - np.min(data, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3rXzRwdPZs_",
        "colab_type": "text"
      },
      "source": [
        "## Noizy Data\n",
        "\n",
        "대부분의 데이터들이 한 곳에 모여있는데, 어떤 일부의 데이터가 그것과 멀리 있어서\n",
        "전체 데이터에 영향을 주고 있을 때 적용\n",
        "\n",
        "NLP에서 필요한 부분만 뽑기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA2sgC6jPl7h",
        "colab_type": "text"
      },
      "source": [
        "# Overfitting\n",
        "\n",
        "학습 데이터를 가지고 했을 때는 정확도가 높은데, 그 학습 데이터에 너무 맞춰져 있어서\n",
        "테스트 데이터로 예측 했을 때 정확도가 낮아지는 현상\n",
        "\n",
        "* High bias -> underfit\n",
        "* High variance -> overfit\n",
        "\n",
        "## 해결 방법\n",
        "\n",
        "* 학습 데이터를 많이 넣는다. - 더욱더 일반화가 된 자료를 사용\n",
        "* 피쳐의 수를 줄인다. - 2차원 -> 1차원 같은 느낌 (weight 수 줄이기)\n",
        "* 피쳐의 수를 늘린다. - high bias일 때. 너무 가설이 단순할 때\n",
        "\n",
        "=> 피쳐 수를 적절히 잡는 것이 중요하다.\n",
        "\n",
        "### Regularization (Add term to loss)\n",
        "람다: Regularization strength\n",
        "$$\n",
        "c = c + \\lambda \\sum W^2\n",
        "$$\n",
        "\n",
        "Resularization Strength를 크게 잡으면 underfit\n",
        "작게 잡으면 overfit\n",
        "\n",
        "적당한 값을 찾는 것이 문제이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77TpetR75RC8",
        "colab_type": "text"
      },
      "source": [
        "# Training set, Test set\n",
        "\n",
        "주어진 데이터를 모두 가지고 학습을 하면 100% 외우는 효과 발생,\n",
        "\n",
        "완벽한 정확도를 얻기 어려움\n",
        "\n",
        "7:3 정도로 나누어서 학습\n",
        "\n",
        "# Training, Validation, Test\n",
        "\n",
        "Validation set으로 alpha, lambda 등 값을 바꾸면서 어떤게 효율적인지...\n",
        "\n",
        "# Online Learning\n",
        "데이터를 순차적으로, 또는 묶어서 주입하여 학습\n",
        "-- Batch와는 달리 데이터가 지속적으로 들어옴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxHY79Pv7j9-",
        "colab_type": "text"
      },
      "source": [
        "# LAB\n",
        "\n",
        "## Without minmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0c-xy027m_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3801f657-0087-4522-d994-e9300520d25d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]],\n",
        "                dtype=np.float32)\n",
        "X = data[:, 0:-1]\n",
        "Y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal([X.shape[1], Y.shape[1]]))\n",
        "b = tf.Variable(tf.random.normal([Y.shape[1]]))\n",
        "\n",
        "for i in range(100+1):\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = tf.matmul(X, W) + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  W.assign_sub(W_grad * 1e-5)\n",
        "  b.assign_sub(b_grad * 1e-5)\n",
        "\n",
        "  if i % 20 == 0:\n",
        "    print(\"{:3} | {:7.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 | 5281608630272.0000\n",
            " 20 |     nan\n",
            " 40 |     nan\n",
            " 60 |     nan\n",
            " 80 |     nan\n",
            "100 |     nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ySgS_zO_OWJ",
        "colab_type": "text"
      },
      "source": [
        "## With minmax - Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gczUXcOr-Yjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8ff6d17f-d7f9-4870-edee-b95ff86dc914"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def min_max_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "data = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]],\n",
        "                dtype=np.float32)\n",
        "\n",
        "# very important. It does not work without it.\n",
        "data = min_max_scaler(data)\n",
        "\n",
        "X = data[:, 0:-1]\n",
        "Y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal([X.shape[1], Y.shape[1]]))\n",
        "b = tf.Variable(tf.random.normal([Y.shape[1]]))\n",
        "\n",
        "for i in range(100+1):\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = tf.matmul(X, W) + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  W.assign_sub(W_grad * 3e-1)\n",
        "  b.assign_sub(b_grad * 3e-1)\n",
        "\n",
        "  if i % 20 == 0:\n",
        "    print(\"{:3} | {:7.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 |  1.4285\n",
            " 20 |  0.0258\n",
            " 40 |  0.0075\n",
            " 60 |  0.0056\n",
            " 80 |  0.0051\n",
            "100 |  0.0050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uo43Mzk_SrJ",
        "colab_type": "text"
      },
      "source": [
        "## With Regularization, Decay lr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgabT3DS_bD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b0f877ca-7609-4408-fc94-f2c792c68955"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def min_max_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "data = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]],\n",
        "                dtype=np.float32)\n",
        "\n",
        "# very important. It does not work without it.\n",
        "data = min_max_scaler(data)\n",
        "\n",
        "X = data[:, 0:-1]\n",
        "Y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal([X.shape[1], Y.shape[1]]))\n",
        "b = tf.Variable(tf.random.normal([Y.shape[1]]))\n",
        "\n",
        "lr = 0.5\n",
        "\n",
        "for i in range(100+1):\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = tf.matmul(X, W) + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    W_reg = tf.nn.l2_loss(W)\n",
        "    cost = tf.reduce_mean(cost + W_reg * 0.01)\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  W.assign_sub(W_grad * lr)\n",
        "  b.assign_sub(b_grad * lr)\n",
        "\n",
        "  if i % 20 == 0:\n",
        "    lr *= 0.96\n",
        "    print(\"{:3} | {:7.4f} | {:3.4f}\".format(i, cost.numpy(), lr))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 |  0.7742 | 0.4800\n",
            " 20 |  0.0114 | 0.4608\n",
            " 40 |  0.0088 | 0.4424\n",
            " 60 |  0.0081 | 0.4247\n",
            " 80 |  0.0075 | 0.4077\n",
            "100 |  0.0071 | 0.3914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZV6sELfBrqb",
        "colab_type": "text"
      },
      "source": [
        "# MNIST - with keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuA7_XgQBwmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2e9bd8e8-ece6-47b4-a6e3-5bf7f11bd939"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XP8SovGCCP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "model = keras.Sequential([\n",
        "  keras.layers.Input((28, 28)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(128, 'relu'),\n",
        "  keras.layers.Dense(len(class_names), 'softmax'),\n",
        "])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7RZ_dl0C0CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "3c17b236-d8d9-4134-ebfa-4efb4afe52e7"
      },
      "source": [
        "model.compile('adam', 'sparse_categorical_crossentropy', ['acc'])\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, None, 5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.7099 - acc: 0.6812\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7118 - acc: 0.7319\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6236 - acc: 0.7592\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5880 - acc: 0.7764\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5639 - acc: 0.7872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f14e892f588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpKXSgqFDV7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3cd1c92-ff14-4f9e-f46b-21fa3851f883"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.5755 - acc: 0.7929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5755416750907898, 0.792900025844574]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZgc5qItDqC0",
        "colab_type": "text"
      },
      "source": [
        "# IMDB\n",
        "\n",
        "0: neg\n",
        "1: pos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeDNWnR3Dr4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=10000)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htywRtQFJkII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52mzYMEFJnyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = {k:v+3 for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "reversed_word_index = dict([(v, k) for (k, v) in word_index.items()])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB0qs77eKuqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                    value=word_index[\"<PAD>\"], padding='post', maxlen=256)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                    value=word_index[\"<PAD>\"], padding='post', maxlen=256)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLG46pCELAc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(10000, 16),\n",
        "  keras.layers.GlobalAveragePooling1D(),\n",
        "  keras.layers.Dense(16, 'relu'),\n",
        "  keras.layers.Dense(1, 'sigmoid'),\n",
        "])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRTIRg1jMvFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "7170c3bf-dc6f-4f21-af61-078613091455"
      },
      "source": [
        "model.compile('adam', 'binary_crossentropy', ['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ngw2aLgM2VU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "472cdd63-3365-47eb-8a9b-4d95b8fe214a"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "history = model.fit(partial_x_train, partial_y_train, 512, 40,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.6923 - acc: 0.5675 - val_loss: 0.6908 - val_acc: 0.6400\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.6878 - acc: 0.6628 - val_loss: 0.6836 - val_acc: 0.7087\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.6758 - acc: 0.7451 - val_loss: 0.6679 - val_acc: 0.7167\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.6540 - acc: 0.7562 - val_loss: 0.6427 - val_acc: 0.7624\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.6212 - acc: 0.7859 - val_loss: 0.6076 - val_acc: 0.7814\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5790 - acc: 0.8083 - val_loss: 0.5661 - val_acc: 0.8014\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5314 - acc: 0.8301 - val_loss: 0.5223 - val_acc: 0.8197\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.4832 - acc: 0.8489 - val_loss: 0.4799 - val_acc: 0.8319\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.4387 - acc: 0.8591 - val_loss: 0.4425 - val_acc: 0.8447\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3989 - acc: 0.8737 - val_loss: 0.4113 - val_acc: 0.8538\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.3651 - acc: 0.8832 - val_loss: 0.3858 - val_acc: 0.8584\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3371 - acc: 0.8897 - val_loss: 0.3663 - val_acc: 0.8623\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.3133 - acc: 0.8964 - val_loss: 0.3491 - val_acc: 0.8677\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.2925 - acc: 0.9019 - val_loss: 0.3357 - val_acc: 0.8706\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.2748 - acc: 0.9060 - val_loss: 0.3245 - val_acc: 0.8756\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.2592 - acc: 0.9112 - val_loss: 0.3157 - val_acc: 0.8777\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2450 - acc: 0.9157 - val_loss: 0.3088 - val_acc: 0.8775\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2321 - acc: 0.9205 - val_loss: 0.3031 - val_acc: 0.8805\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2204 - acc: 0.9249 - val_loss: 0.2980 - val_acc: 0.8821\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.2100 - acc: 0.9282 - val_loss: 0.2941 - val_acc: 0.8819\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1999 - acc: 0.9322 - val_loss: 0.2910 - val_acc: 0.8822\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.1908 - acc: 0.9359 - val_loss: 0.2890 - val_acc: 0.8838\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1828 - acc: 0.9416 - val_loss: 0.2870 - val_acc: 0.8843\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1743 - acc: 0.9445 - val_loss: 0.2868 - val_acc: 0.8836\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1669 - acc: 0.9477 - val_loss: 0.2848 - val_acc: 0.8849\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1597 - acc: 0.9500 - val_loss: 0.2848 - val_acc: 0.8847\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1532 - acc: 0.9525 - val_loss: 0.2849 - val_acc: 0.8851\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1468 - acc: 0.9555 - val_loss: 0.2852 - val_acc: 0.8859\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1408 - acc: 0.9580 - val_loss: 0.2860 - val_acc: 0.8852\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1355 - acc: 0.9599 - val_loss: 0.2881 - val_acc: 0.8841\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.1299 - acc: 0.9615 - val_loss: 0.2914 - val_acc: 0.8818\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1252 - acc: 0.9647 - val_loss: 0.2904 - val_acc: 0.8852\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1209 - acc: 0.9649 - val_loss: 0.2943 - val_acc: 0.8831\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1157 - acc: 0.9679 - val_loss: 0.2947 - val_acc: 0.8837\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.1111 - acc: 0.9697 - val_loss: 0.2969 - val_acc: 0.8841\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1067 - acc: 0.9714 - val_loss: 0.2996 - val_acc: 0.8839\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.1034 - acc: 0.9716 - val_loss: 0.3033 - val_acc: 0.8829\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0991 - acc: 0.9741 - val_loss: 0.3056 - val_acc: 0.8831\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0952 - acc: 0.9759 - val_loss: 0.3097 - val_acc: 0.8815\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0918 - acc: 0.9769 - val_loss: 0.3124 - val_acc: 0.8819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9KnJnE9Ny4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c30c2bcb-8cba-453d-9455-5e165f3bed6d"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3322 - acc: 0.8735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3321771025657654, 0.873520016670227]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi7owTD7OBt4",
        "colab_type": "text"
      },
      "source": [
        "# CIFAR-100"
      ]
    }
  ]
}